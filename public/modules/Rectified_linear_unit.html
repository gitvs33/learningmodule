<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Activation Function Explainer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Roboto+Mono:wght@400&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc; /* slate-50 */
        }
        .font-mono {
            font-family: 'Roboto Mono', monospace;
        }
        .active-btn {
            background-color: #4f46e5; /* indigo-600 */
            color: white;
            font-weight: 600;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
        }
        .info-card {
            background: white;
            border-radius: 0.75rem;
            border: 1px solid #e2e8f0; /* slate-200 */
            box-shadow: 0 1px 3px 0 rgb(0 0 0 / 0.05);
            transition: all 0.3s ease-in-out;
        }
        .data-point {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            position: absolute;
            transition: all 0.3s ease;
        }
    </style>
</head>
<body class="text-slate-800">

    <div class="container mx-auto p-4 md:p-8">
        <header class="text-center mb-12">
            <h1 class="text-4xl md:text-5xl font-bold text-slate-900">The Power of Activation Functions</h1>
            <p class="mt-4 text-lg text-slate-600 max-w-3xl mx-auto">The 'gatekeeper' inside every neuron that decides what information to pass on.</p>
        </header>

        <!-- Main Interactive Section -->
        <section class="info-card p-6 md:p-8 mb-12">
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                <!-- Graph and Controls -->
                <div>
                    <h2 class="text-2xl font-bold mb-4">Visualize the Activation</h2>
                    <div class="mb-4">
                        <canvas id="activationChart"></canvas>
                    </div>
                    <div class="flex items-center gap-4">
                        <label for="neuronInput" class="font-medium">Neuron Input (x):</label>
                        <input type="range" id="neuronInput" min="-5" max="5" step="0.1" value="2" class="w-full">
                    </div>
                     <div class="mt-4 text-center bg-slate-100 p-4 rounded-lg">
                        <p class="text-lg">Input: <span id="inputValue" class="font-bold font-mono">2.0</span></p>
                        <p class="text-lg">Output: <span id="outputValue" class="font-bold font-mono text-indigo-600">2.0</span></p>
                    </div>
                </div>
                <!-- Information Panel -->
                <div>
                    <div class="flex flex-wrap gap-2 mb-6">
                        <button class="activation-btn active-btn px-4 py-2 rounded-full transition" data-func="relu">ReLU</button>
                        <button class="activation-btn px-4 py-2 rounded-full transition" data-func="sigmoid">Sigmoid</button>
                        <button class="activation-btn px-4 py-2 rounded-full transition" data-func="tanh">Tanh</button>
                        <button class="activation-btn px-4 py-2 rounded-full transition" data-func="linear">Linear (None)</button>
                    </div>
                    <div id="info-panel">
                        <h3 id="info-title" class="text-2xl font-bold mb-2"></h3>
                        <p id="info-description" class="text-slate-600 mb-4"></p>
                        <div class="bg-slate-50 border border-slate-200 rounded-lg p-4">
                            <h4 class="font-semibold text-slate-700 mb-2">Key Characteristics:</h4>
                            <ul id="info-pros" class="list-disc list-inside space-y-1 text-sm text-slate-600"></ul>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Why it's Necessary Section -->
        <section class="info-card p-6 md:p-8">
            <h2 class="text-3xl font-bold text-center mb-2">Why Are They Necessary?</h2>
            <p class="text-lg text-slate-600 max-w-3xl mx-auto text-center mb-8">To introduce <span class="font-bold text-indigo-600">non-linearity</span>. Without it, a neural network can only learn simple, straight-line patterns.</p>
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <!-- Visualization -->
                <div class="relative w-full aspect-square bg-slate-100 rounded-lg border border-slate-200">
                    <!-- Data points will be added here -->
                    <div id="data-container"></div>
                    <!-- Decision boundary line -->
                    <svg id="decision-boundary" class="absolute top-0 left-0 w-full h-full" viewbox="0 0 100 100" preserveAspectRatio="none">
                        <path d="M 0 50 L 100 50" stroke="#ef4444" stroke-width="1" stroke-dasharray="2 2" />
                    </svg>
                </div>
                <!-- Explanation and Controls -->
                <div>
                    <h3 class="text-xl font-semibold mb-4">The Classification Challenge</h3>
                    <p class="text-slate-600 mb-4">Imagine we need to separate the blue dots from the orange dots. A simple straight line can't do the job.</p>
                    <div class="flex items-center gap-4 bg-white p-4 rounded-lg border">
                        <label for="activationToggle" class="font-medium">Use Activation Function (ReLU):</label>
                        <div class="relative inline-block w-12 mr-2 align-middle select-none transition duration-200 ease-in">
                            <input type="checkbox" name="toggle" id="activationToggle" class="toggle-checkbox absolute block w-7 h-7 rounded-full bg-white border-4 appearance-none cursor-pointer"/>
                            <label for="activationToggle" class="toggle-label block overflow-hidden h-7 rounded-full bg-gray-300 cursor-pointer"></label>
                        </div>
                    </div>
                    <p id="analogy-result" class="mt-4 text-center font-semibold text-lg p-3 rounded-lg bg-red-100 text-red-700">
                        FAIL: The network can only draw a straight line.
                    </p>
                </div>
            </div>
        </section>
    </div>

    <style>
        .toggle-checkbox:checked {
            right: 0;
            border-color: #4f46e5;
        }
        .toggle-checkbox:checked + .toggle-label {
            background-color: #4f46e5;
        }
    </style>

    <script>
        const activationFunctions = {
            relu: x => Math.max(0, x),
            sigmoid: x => 1 / (1 + Math.exp(-x)),
            tanh: x => Math.tanh(x),
            linear: x => x
        };

        const info = {
            relu: {
                title: "ReLU (Rectified Linear Unit)",
                description: "The most popular activation function in deep learning. It's simple and efficient. If the input is positive, it returns the input; otherwise, it returns 0.",
                pros: [
                    "Computationally very efficient.",
                    "Helps prevent the vanishing gradient problem.",
                    "Introduces sparsity in the network."
                ]
            },
            sigmoid: {
                title: "Sigmoid",
                description: "A classic 'S'-shaped curve. It squashes any input value into a range between 0 and 1. Useful for binary classification output layers.",
                pros: [
                    "Smooth gradient prevents sudden jumps in output.",
                    "Output is always between 0 and 1 (interpretable as a probability).",
                    "Prone to the vanishing gradient problem."
                ]
            },
            tanh: {
                title: "Tanh (Hyperbolic Tangent)",
                description: "Similar to Sigmoid, but squashes values into a range between -1 and 1. It's zero-centered, which can help in model training.",
                pros: [
                    "Zero-centered output can speed up convergence.",
                    "Steeper gradient than Sigmoid.",
                    "Also suffers from the vanishing gradient problem."
                ]
            },
            linear: {
                title: "Linear (No Activation)",
                description: "This is essentially 'no activation'. The output is identical to the input. A network with only linear layers can only learn linear relationships.",
                pros: [
                    "Used in the output layer for regression tasks (predicting a continuous value).",
                    "Cannot be used in hidden layers for complex problems.",
                    "Does not introduce non-linearity."
                ]
            }
        };

        let myChart;
        let currentFunction = 'relu';

        const neuronInput = document.getElementById('neuronInput');
        const inputValue = document.getElementById('inputValue');
        const outputValue = document.getElementById('outputValue');
        const activationButtons = document.querySelectorAll('.activation-btn');

        function updateInfoPanel(funcName) {
            const data = info[funcName];
            document.getElementById('info-title').textContent = data.title;
            document.getElementById('info-description').textContent = data.description;
            const prosList = document.getElementById('info-pros');
            prosList.innerHTML = '';
            data.pros.forEach(pro => {
                const li = document.createElement('li');
                li.textContent = pro;
                prosList.appendChild(li);
            });
        }

        function updateChart() {
            const inputVal = parseFloat(neuronInput.value);
            const outputVal = activationFunctions[currentFunction](inputVal);

            inputValue.textContent = inputVal.toFixed(2);
            outputValue.textContent = outputVal.toFixed(2);

            myChart.data.datasets[1].data = [{ x: inputVal, y: outputVal }];
            myChart.update();
        }

        function createChart() {
            const ctx = document.getElementById('activationChart').getContext('2d');
            const data = {
                labels: Array.from({ length: 101 }, (_, i) => (i - 50) / 10),
                datasets: [{
                    label: currentFunction,
                    data: Array.from({ length: 101 }, (_, i) => activationFunctions[currentFunction]((i - 50) / 10)),
                    borderColor: '#4f46e5',
                    borderWidth: 2,
                    fill: false,
                    tension: 0.4,
                    pointRadius: 0
                }, {
                    label: 'Current Output',
                    data: [],
                    backgroundColor: '#db2777', // pink-600
                    pointRadius: 6,
                    pointHoverRadius: 8
                }]
            };

            myChart = new Chart(ctx, {
                type: 'line',
                data: data,
                options: {
                    responsive: true,
                    scales: {
                        x: {
                            title: { display: true, text: 'Input (x)' }
                        },
                        y: {
                            title: { display: true, text: 'Output (f(x))' },
                            min: -1.5,
                            max: 5.5
                        }
                    },
                    plugins: {
                        legend: { display: false }
                    }
                }
            });
        }

        activationButtons.forEach(button => {
            button.addEventListener('click', () => {
                activationButtons.forEach(btn => btn.classList.remove('active-btn'));
                button.classList.add('active-btn');
                currentFunction = button.dataset.func;
                myChart.data.datasets[0].label = currentFunction;
                myChart.data.datasets[0].data = myChart.data.labels.map(x => activationFunctions[currentFunction](x));
                
                // Adjust y-axis for better visualization
                if(currentFunction === 'sigmoid') {
                    myChart.options.scales.y.min = -0.1;
                    myChart.options.scales.y.max = 1.1;
                } else if (currentFunction === 'tanh') {
                    myChart.options.scales.y.min = -1.1;
                    myChart.options.scales.y.max = 1.1;
                } else {
                     myChart.options.scales.y.min = -1.5;
                     myChart.options.scales.y.max = 5.5;
                }

                updateInfoPanel(currentFunction);
                updateChart();
            });
        });

        neuronInput.addEventListener('input', updateChart);
        
        // --- Analogy Section ---
        const analogyToggle = document.getElementById('activationToggle');
        const analogyResult = document.getElementById('analogy-result');
        const decisionBoundary = document.getElementById('decision-boundary').querySelector('path');

        const dataPoints = [
            { x: 20, y: 25, class: 'blue' }, { x: 30, y: 20, class: 'blue' },
            { x: 40, y: 30, class: 'blue' }, { x: 15, y: 40, class: 'blue' },
            { x: 80, y: 75, class: 'blue' }, { x: 90, y: 80, class: 'blue' },
            { x: 85, y: 65, class: 'blue' }, { x: 75, y: 85, class: 'blue' },
            { x: 50, y: 50, class: 'orange' }, { x: 60, y: 45, class: 'orange' },
            { x: 45, y: 60, class: 'orange' }, { x: 55, y: 55, class: 'orange' }
        ];

        function renderDataPoints() {
            const container = document.getElementById('data-container');
            container.innerHTML = '';
            dataPoints.forEach(p => {
                const el = document.createElement('div');
                el.className = 'data-point';
                el.style.left = `${p.x}%`;
                el.style.top = `${p.y}%`;
                el.style.backgroundColor = p.class === 'blue' ? '#3b82f6' : '#f97316';
                container.appendChild(el);
            });
        }
        
        analogyToggle.addEventListener('change', () => {
            if (analogyToggle.checked) {
                // With activation
                decisionBoundary.setAttribute('d', 'M 0 65 L 35 65 C 40 65, 40 35, 45 35 L 65 35 C 70 35, 70 65, 75 65 L 100 65');
                decisionBoundary.style.stroke = '#22c55e'; // green-500
                analogyResult.textContent = "SUCCESS: The network can create a non-linear boundary!";
                analogyResult.classList.remove('bg-red-100', 'text-red-700');
                analogyResult.classList.add('bg-green-100', 'text-green-700');
            } else {
                // Without activation (linear)
                decisionBoundary.setAttribute('d', 'M 0 50 L 100 50');
                decisionBoundary.style.stroke = '#ef4444'; // red-500
                analogyResult.textContent = "FAIL: The network can only draw a straight line.";
                analogyResult.classList.remove('bg-green-100', 'text-green-700');
                analogyResult.classList.add('bg-red-100', 'text-red-700');
            }
        });

        // --- Initial Load ---
        window.onload = () => {
            createChart();
            updateInfoPanel('relu');
            updateChart();
            renderDataPoints();
        };

    </script>
</body>
</html>
